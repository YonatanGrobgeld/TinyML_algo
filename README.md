## Project Overview

This repository contains TinyML algorithms and kernels with a focus on deployment on FPGA, RISC-V, and LiteX-based systems. The code is derived from and extends the original PULP transformer kernels to support additional platforms and microcontroller-style execution environments.

## TinyFormer on VexRiscv + LiteX (FPGA)

A TinyFormer encoder block (Transformer-style TinyML model) is implemented for a bare-metal RV32IM core running on VexRiscv generated by LiteX, targeted at FPGA platforms such as the Nexys 4 DDR.

The implementation under `litex_port/` is designed to:

- **Target**: bare-metal RV32IM (no OS, no threads) on VexRiscv generated by LiteX.
- **Platform**: tested/targeted for Nexys 4 DDR FPGA (LiteX SoC).
- **Portability**: use only standard integer types and fixed-size buffers so it can be reused on other RV32-class MCUs and SoCs.

## TinyFormer Encoder Algorithm

The TinyFormer encoder block implemented here follows the standard Transformer encoder structure, specialized for int8 inference and streaming attention:

- **Q/K/V linear projections**:  
  Input tokens `X` with shape \(S \times D\) (sequence length \(S = 16\), model dimension \(D = 32\)) are projected to queries, keys, and values using int8 weight matrices and int32 accumulators.

- **Streaming (tiled) scaled dot-product attention**:  
  For each query position, dot products with all keys are accumulated in int32, scaled, and then converted to attention scores. The implementation **does not** allocate an explicit \(S \times S\) attention matrix; instead it processes one query at a time and reuses 1D scratch buffers.

- **Max-subtraction softmax with small LUT**:  
  For numerical stability, the maximum score for each query is subtracted before a fixed-point softmax. A small integer lookup table approximates \(\exp(x)\) over a limited range, and the resulting values are normalized using a Q15-style fixed-point representation.

- **Context computation**:  
  The attention output for each token is obtained as a weighted sum of the value vectors using the normalized softmax weights.

- **Output projection**:  
  The attention context is passed through a final linear projection (int8 weights, int32 accumulators, saturated back to int8).

- **Residual connections**:  
  A residual connection is applied around the attention block (`Y = X + AttnOut`) and around the feed-forward network (`Z = Y + FFNOut`), with results saturated back to int8.

- **Feed-Forward Network (ReLU)**:  
  A two-layer position-wise feed-forward network with ReLU activation in the hidden layer:
  - First layer: \(D \rightarrow \text{FFN}\) (here \(\text{FFN} = 64\))
  - Second layer: \(\text{FFN} \rightarrow D\)
  Both layers use int8 weights with int32 accumulators and shift/saturate back to int8.

- **Quantization**:  
  All weights and activations are int8, with int32 accumulators and simple right-shift scaling. The current implementation uses placeholder all-zero weights which can be replaced by trained, quantized parameters.

## Memory & System Constraints

The TinyFormer implementation is designed for constrained, microcontroller-class environments:

- **Bare-metal**: no operating system, no threads, no dynamic loading.
- **No dynamic allocation**: there is no `malloc` or `free`; all buffers are statically allocated with fixed sizes.
- **Fixed-size buffers only**: sequence length \(S = 16\), model dimension \(D = 32\), and feed-forward width \(\text{FFN} = 64\) are compile-time constants.
- **Microcontroller-class SRAM usage**: global working buffers and I/O tensors require only a few kilobytes of RAM, making the kernel suitable for small RV32IM MCUs and soft cores on FPGA.
- **Hardware-acceleration friendly**: the math kernels are written in portable C, with clear matmul and attention loops that can be offloaded or accelerated in FPGA logic at a later stage.

## Directory Structure

At the top level of this repository:

- `pulp-transformer/` – Original PULP transformer kernels, tests, and documentation.
- `litex_port/` – TinyFormer encoder and bare-metal test harness for VexRiscv + LiteX.

The `litex_port/` directory contains:

- `tinyformer.h` – Model parameters and public API for the TinyFormer encoder block (fixed `S`, `D`, and FFN width).
- `tinyformer.c` – TinyFormer encoder implementation:
  - Q/K/V projections
  - Streaming scaled dot-product attention with max-subtraction softmax and LUT
  - Output projection and residual connection
  - Feed-forward network (ReLU) and final residual
  - Int8 weights/activations, int32 accumulators
- `main.c` – Bare-metal test harness:
  - Initializes a deterministic int8 input tensor
  - Runs the TinyFormer encoder
  - Computes a checksum over the output tensor
  - Sends the checksum over UART using simple `uart_write_*` helpers

## How to Run on LiteX

1. **Integrate sources into your LiteX firmware build**  
   Add the following files to your LiteX firmware or standalone bare-metal build:
   - `litex_port/tinyformer.c`
   - `litex_port/tinyformer.h`
   - `litex_port/main.c`

2. **Wire UART stubs to LiteX UART MMIO**  
   In `litex_port/main.c`, the function:
   - `uart_write_char(char c)`  
   is currently a stub and must be implemented using your LiteX UART registers (e.g. `UART_RXTX` and `UART_TXFULL`). `uart_write_string` and `uart_write_hex32` are built on top of `uart_write_char` and will work once this function is wired.

3. **Build as a LiteX bare-metal kernel/firmware**  
   Use your existing LiteX build flow to compile the firmware for VexRiscv (RV32IM). The result should be an ELF file, for example:
   - `firmware.elf`

4. **Load and run with `litex_term`**  
   With the FPGA bitstream and LiteX SoC loaded onto the Nexys 4 DDR:

   ```bash
   litex_term --kernel firmware.elf
   ```

   The TinyFormer test harness will run on reset, execute a single encoder block, and print a checksum line on the LiteX UART console similar to:

   ```text
   TinyFormer checksum: 0xXXXXXXXX
   ```

   This checksum can be compared against a host-side reference implementation to verify correctness.

## Next Steps

Suggested next engineering steps:

- **UART integration**:  
  Replace the body of `uart_write_char` in `litex_port/main.c` with real LiteX UART MMIO operations so that the checksum is visible on the serial console.

- **Load real weights**:  
  Replace the placeholder zero-initialized weight and bias arrays in `litex_port/tinyformer.c` with trained, quantized int8 parameters exported from your training pipeline.

- **Add cycle counting**:  
  Instrument performance by reading the RISC-V cycle counter (`rdcycle`) before and after `tinyformer_encode` to measure latency on VexRiscv.

- **Hardware acceleration**:  
  Identify the critical matmul and attention loops in `tinyformer.c` and consider offloading them to FPGA accelerators or tightly coupled custom instructions, while preserving the same C API.

- **Extended testing**:  
  Add additional test cases and golden-reference comparisons (e.g., host-side Python or C reference) to validate the TinyFormer outputs across different inputs and weight configurations.

## Training + Weight Export

To use trained TinyFormer weights with the C inference kernel in `litex_port/`:

- **Training checkpoint**:  
  Train your TinyFormer encoder (S=16, D=32, FFN=64, 1 head) in PyTorch and save a `state_dict` containing at least the following keys:
  - `W_q`, `W_k`, `W_v`, `W_o` with shape `[32, 32]`
  - `W_ff1` with shape `[64, 32]` or `[32, 64]`
  - `W_ff2` with shape `[32, 64]` or `[64, 32]`
  - `b_q`, `b_k`, `b_v`, `b_o` with shape `[32]`
  - `b_ff1` with shape `[64]`
  - `b_ff2` with shape `[32]`

- **Export to C weights**:  
  From the repository root (`TinyML_algo/`), run:

  ```bash
  python3 tools/export_weights.py \
      --checkpoint path/to/state_dict.pt \
      --output-dir litex_port
  ```

  This generates:

  - `litex_port/trained_weights.h`
  - `litex_port/trained_weights.c`

  containing quantized `int8_t` weights and biases that match the layout expected by `litex_port/tinyformer.c`.

- **Enabling trained weights in C**:  
  The TinyFormer implementation supports a compile-time switch:

  ```c
  #define USE_TRAINED_WEIGHTS 1
  ```

  When `USE_TRAINED_WEIGHTS` is set to `1` at compile time (e.g. via compiler flags or a config header), `tinyformer.c` includes `trained_weights.h` and uses the exported arrays instead of the built-in zero-initialized placeholders. When it is `0` (default), the original placeholder weights are used.

## UCI HAR End-to-End Demo

This repository includes an end-to-end pipeline that trains a TinyFormer-based classifier on the UCI HAR dataset (using the raw inertial signals) and exports both:
- Quantized TinyFormer encoder weights for the C implementation in `litex_port/tinyformer.c`, and
- A small set of preprocessed demo samples that can be run on an FPGA target (VexRiscv + LiteX).

### Steps to run the full pipeline

From the repository root (`TinyML_algo/`), run:

```bash
python3 training/download_uci_har.py
python3 training/preprocess_uci_har.py
python3 training/train_tinyformer_uci_har.py
python3 training/export_and_make_fpga_demo.py
```

- `download_uci_har.py` downloads and extracts the UCI HAR dataset into `data/uci_har/`.
- `preprocess_uci_har.py` loads the raw inertial signals (6 channels × 128 timesteps), downsamples to 16 timesteps using average pooling, constructs 32-dimensional feature vectors per timestep, normalizes features using train mean/std, and saves `data/uci_har_processed.npz`.
- `train_tinyformer_uci_har.py` trains a TinyFormer encoder + classifier head (S=16, D=32, FFN=64, 1 head, 6 classes), prints train/test accuracy, and saves:
  - `artifacts/state_dict.pt` (TinyFormer encoder weights with keys `W_q`, `W_k`, `W_v`, `W_o`, `W_ff1`, `W_ff2`, `b_q`, `b_k`, `b_v`, `b_o`, `b_ff1`, `b_ff2`)
  - `artifacts/classifier.npz` (classifier head weights `W_cls[6,32]`, `b_cls[6]`).
- `export_and_make_fpga_demo.py`:
  - Runs `tools/export_weights.py` on `artifacts/state_dict.pt` to create `litex_port/trained_weights.c/h`.
  - Selects a small set of test samples, quantizes them to int8, and writes `litex_port/demo_samples.c/h`.
  - Quantizes the classifier head weights and writes `litex_port/demo_classifier.c/h`.

### Running the FPGA demo

On the LiteX/VexRiscv target (e.g., Nexys 4 DDR):

1. **Build firmware**  
   Include the following files in your LiteX bare-metal build:
   - `litex_port/tinyformer.c`, `litex_port/tinyformer.h`
   - `litex_port/main.c` (checksum demo) or `litex_port/demo_main.c` (UCI HAR classification demo)
   - `litex_port/trained_weights.c`, `litex_port/trained_weights.h`
   - `litex_port/demo_samples.c`, `litex_port/demo_samples.h`
   - `litex_port/demo_classifier.c`, `litex_port/demo_classifier.h`

   Compile with:

   ```bash
   -DUSE_TRAINED_WEIGHTS=1
   ```

   so that `tinyformer.c` uses the trained weights exported by the Python tooling.

2. **UART wiring**  
   Implement `uart_write_char` in `main.c` or `demo_main.c` to write to the LiteX UART MMIO registers. The helper functions `uart_write_string` and the demo printing logic will then send checksums or predicted labels to the serial console.

3. **Run via litex_term**  
   After building the firmware ELF, run it on the FPGA using:

   ```bash
   litex_term --kernel firmware.elf
   ```

   - `main.c` will print a checksum of a single TinyFormer encoder pass.
   - `demo_main.c` will iterate over the generated demo samples, run the TinyFormer encoder + classifier head, and print predicted vs expected activity labels for quick on-board validation.



